{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task4.ipynb","provenance":[{"file_id":"1u9L3sZLxKuzkkRgpd8_rM7kWielHiRq5","timestamp":1577648508933},{"file_id":"10jQIVccWKMTEfqbu6UlopS0Q4OaidF_x","timestamp":1577646953873}],"collapsed_sections":["UJgjqQfn7Mo4","IKwE1JaGLCET","KJsGJ4OmFs0o"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"e6D7lmOUUgg8","colab_type":"text"},"source":["# Task 4\n"]},{"cell_type":"code","metadata":{"id":"NKg9omoXwnfO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"8308f1f5-13af-45ba-a4d7-d6e56cfc9e76","executionInfo":{"status":"ok","timestamp":1578754106744,"user_tz":-60,"elapsed":8756,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow import keras\n","import os\n","from google.colab import drive\n","import numpy as np"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","2.1.0-rc1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ATgiy_z6B0IQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"5f57ff8b-b0cc-4c7e-9741-b025638633f0","executionInfo":{"status":"ok","timestamp":1578754138925,"user_tz":-60,"elapsed":25665,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["import os\n","from google.colab import drive\n","import numpy as np\n","drive.mount('/content/drive')\n","os.listdir()\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['.config', 'drive', 'sample_data']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"zg-pz2q7tnNd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"outputId":"69a884dd-7eec-48c1-c20f-1b4d9867063e","executionInfo":{"status":"ok","timestamp":1578754142647,"user_tz":-60,"elapsed":2305,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["base_dir = 'drive/My Drive/Computational Intelligence - MY PROJECT/My_Project_CompInt' \n","os.listdir(base_dir)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['OldNotebooks',\n"," 'Task 1',\n"," 'Models',\n"," 'Task4_RelevantPapers',\n"," 'Tensors',\n"," 'Rasla_Davide_505828',\n"," 'Copia di Task 3.1_ INCEPTIONV3.ipynb',\n"," 'Task4.ipynb']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Iq1-POSf7zHs","colab_type":"text"},"source":["# Getting the Data "]},{"cell_type":"code","metadata":{"id":"EnKD_z8M4B7T","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images, train_labels, test_images, test_lable = load_training()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57CXX2i974iQ","colab_type":"text"},"source":["## Reshape"]},{"cell_type":"code","metadata":{"id":"1NjntIxS76yX","colab_type":"code","colab":{}},"source":["\n","train_images = train_images.reshape((5352, 150, 150,1))\n","train_images = train_images.astype('float32') / 65535\n","\n","\n","\n","\n","test_images = test_images.reshape((672, 150, 150,1))\n","test_images = test_images.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_wjd_hjoHab","colab_type":"code","colab":{}},"source":["print(train_images[0:100])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJgjqQfn7Mo4","colab_type":"text"},"source":["## To categorical"]},{"cell_type":"code","metadata":{"id":"HQTdrLxRi6Md","colab_type":"code","colab":{}},"source":["#Se uso softmax\n","from keras.utils import to_categorical\n","LabelArray_Categorical = to_categorical(train_labels,5)\n","Test_label_categorical = to_categorical(test_lable,5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0uFijPyDZV9","colab_type":"code","colab":{}},"source":["print(LabelArray_Categorical[0:1000])\n","print(LabelArray_Categorical[2000:])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IljNnNoexBiz","colab_type":"text"},"source":["## Modifiyng the test set"]},{"cell_type":"markdown","metadata":{"id":"7I9CwhNZXT8J","colab_type":"text"},"source":["### SPLITTING AND CATEGORICAL"]},{"cell_type":"code","metadata":{"id":"BMYALTE6XXY9","colab_type":"code","colab":{}},"source":["import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","\n","Test_baseline = []\n","Test_abnormality = []\n","Test_baseline_target = []\n","Test_abnormality_target = []\n","                      \n","#splitting\n","for i in range(0, len(test_images)): \n","    if (i % 2)==0: \n","        Test_baseline.append(test_images[i])\n","        Test_baseline_target.append(test_lable[i])\n","    else : \n","        Test_abnormality.append(test_images[i])\n","        Test_abnormality_target.append(test_lable[i])\n","        \n","#editing labels\n","leng = len(Test_abnormality_target)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target)\n","for i in range(0, leng):\n","    if Test_abnormality_target[i] == 1:\n","       Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target[i] == 2:\n","       Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target[i] == 3:\n","       Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target[i] == 4:\n","       Test_abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","Testabnormality_new = np.array(Test_abnormality)\n","Testbaseline_new = np.array(Test_baseline)\n","\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FhHxTa2AQ2P","colab_type":"code","colab":{}},"source":["print(Test_abnormality_target_Categorical)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IB4YjIYpK8kS","colab_type":"text"},"source":["# USING SIAMESE NETWORK FROM SCRATCH\n","\n","Based on: https://keras.io/examples/mnist_siamese/\n"]},{"cell_type":"markdown","metadata":{"id":"IYrJTweRWr5a","colab_type":"text"},"source":["Define the two siamese net\n"]},{"cell_type":"code","metadata":{"id":"TVr_S0X6f5SL","colab_type":"code","colab":{}},"source":["from tensorflow.keras import layers\n","def get_siamese_model(input_shape):\n","\n","\n","    # Define the tensors for the two input images\n","    left_input = keras.layers.Input(input_shape)\n","    right_input = keras.layers.Input(input_shape)\n","    \n","    # Convolutional Neural Network\n","    model = keras.models.Sequential()\n","    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\",input_shape=(150, 150, 1),padding='same',strides=1))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    model.add(layers.Conv2D(64, (3, 3), activation=\"relu\",padding='same',strides=1,kernel_regularizer=keras.regularizers.l2(0.001)))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    model.add(layers.Conv2D(128, (2, 2), activation=\"relu\",padding='same',strides=1,kernel_regularizer=keras.regularizers.l2(0.001)))\n","    model.add(layers.MaxPooling2D((2, 2)))\n","    model.add(layers.Conv2D(256, (2, 2), activation=\"relu\",padding='same',strides=1,kernel_regularizer=keras.regularizers.l2(0.001)))\n","    model.add(layers.Flatten())\n","    # Generate the encodings (feature vectors) for the two images\n","    encoded_l = model(left_input)\n","    encoded_r = model(right_input)\n","    \n","    Subtracted = keras.layers.Subtract()([encoded_l,encoded_r])\n","    dense1 = keras.layers.Dense(256, activation=\"relu\")(Subtracted)\n","    dropout2 = keras.layers.Dropout(0.4)(dense1)\n","    dense2 = keras.layers.Dense(128, activation=\"relu\")(dropout2)\n","    out = keras.layers.Dense(4, activation='softmax')(dense2)\n","\n","\n","    # Connect the inputs with the outputs\n","    siamese_net = keras.models.Model(inputs=[left_input,right_input],outputs=out)\n","    \n","    # return the model\n","    return siamese_net\n","\n","input_shape = (150,150,1)\n","\n","model  = get_siamese_model (input_shape)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xYxaVZ7uFmxc","colab":{}},"source":["model.compile(optimizer='Adam',\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EOr1yq0EPzl-","colab_type":"text"},"source":["## Data Manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_wSJ80PY53c3","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","train_images_Augmented = train_images_Augmented.astype('float32') / 65535\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMwj9eqpyLlX","colab_type":"code","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aa81j8McG-ef","colab":{}},"source":["#DATA\n","import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYSgPhIBmIoG","colab_type":"code","colab":{}},"source":["print(abnormality_Not_Shuffled[3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KJsGJ4OmFs0o"},"source":["## Cross_validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LLCLaXfdFs0p","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","def plot():\n","  acc = history.history['categorical_accuracy']\n","  val_acc = history.history['val_categorical_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()\n","  return\n","\n","\n","############################# CROSS VALIDATION ######################\n","\n","k = 10\n","\n","num_epochs = 40\n","History_Arr =[]\n","input_shape = (150,150,1)\n","Cross_model = get_siamese_model(input_shape)\n","Cross_model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","History_Arr =[]\n","\n","print(len(abnormality))\n","print(len(baseline))\n","print(abnormality_target_Categorical)\n","\n","\n","num_val_samples = len(baseline) // k\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data_ABNORMALITY =  abnormality[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets_ABNORMALITY = abnormality_target_Categorical[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_data_BASELINE = baseline[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data_ABN =    np.concatenate([abnormality[:i * num_val_samples],abnormality[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets_ABN = np.concatenate([abnormality_target_Categorical[:i * num_val_samples],abnormality_target_Categorical[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_data_BASE = np.concatenate([baseline[:i * num_val_samples],baseline[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit([partial_train_data_BASE,partial_train_data_ABN ], partial_train_targets_ABN, validation_data=((val_data_BASELINE,val_data_ABNORMALITY), val_targets_ABNORMALITY), epochs=num_epochs, batch_size=32)\n","  History_Arr.append(history)\n","  Cross_model = get_siamese_model(input_shape)\n","  Cross_model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  plot()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vops0Ya_r840","colab_type":"code","colab":{}},"source":["print(abnormality_new)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"30bxg8QLUvun","colab_type":"text"},"source":["## Fitting"]},{"cell_type":"code","metadata":{"id":"62HI8gAC5v0O","colab_type":"code","colab":{}},"source":[" history = model.fit([baseline,abnormality ], abnormality_target_Categorical, epochs=30\n","                             , batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1AASGgD9BCb","colab_type":"text"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"koWOZyJqvd-6","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = model.evaluate((Test_baseline_Not_Shuffled, Test_abnormality_Not_Shuffled), Test_abnormality_target_Categorical, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bpYeSHtey5mH"},"source":["## Adding Data Augmentation "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"13iZ-KKGy5mL"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tzIzWF96G429","colab":{}},"source":["def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhZOkAg90AYB","colab_type":"text"},"source":["Test manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9SUR9adQp76x","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kxhIX-9lp760","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKZkf9pI0Q15","colab_type":"code","colab":{}},"source":["print(Test_abnormality_Not_Shuffled)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5ur2W3DBM8d","colab_type":"code","colab":{}},"source":["#ABNORMALITY OPERATIONS\n","\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqcAgokMlvk_","colab_type":"code","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def double_generator(train_base_images, train_abn_images, train_labels, subset, batch_size=32):\n","\n","    gen = ImageDataGenerator(\n","      validation_split=0.1,\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.1, \n","      horizontal_flip=True,\n","      vertical_flip = True,\n","      fill_mode='nearest'\n","    )\n","\n","    gen.fit(train_abn_images)\n","\n","    gen_abn = gen.flow(train_abn_images, train_labels,  batch_size=batch_size, subset=subset, seed=1)\n","    gen_base = gen.flow(train_base_images, train_labels, batch_size=batch_size, subset=subset, seed=1)\n","\n","    while True:\n","        abn_img, abn_label = gen_abn.next()\n","        base_img, _ = gen_base.next()\n","        yield [base_img, abn_img], abn_label\n","\n","train_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'training', batch_size=32)\n","validation_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'validation', batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p0DSBGjgy5me"},"source":["### FITTING"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wtsUU7Ioy5mg","colab":{}},"source":["history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=40,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"89YsWI0Vy5mi"},"source":["### Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0oVjUzZEy5mj","colab":{}},"source":["\n","\n","test_loss, test_acc = model.evaluate((Test_baseline_Not_Shuffled, Test_abnormality_Not_Shuffled),Test_abnormality_target_Categorical,batch_size=32,  verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mT_GNtimy5ml"},"source":["### Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SDfO8D7Oy5mm","colab":{}},"source":["#Save the model!\n","model.save(os.path.join(base_dir,'Models/Model_Task_4_Siamese_WithDataAugmentation_0.52.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VSIsm4tjPSYU"},"source":["# USING DCCNN (Deep - Columnar)"]},{"cell_type":"markdown","metadata":{"id":"dytZGFwVQAZb","colab_type":"text"},"source":["## Model "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W7gzQv_xPSYZ","colab":{"base_uri":"https://localhost:8080/","height":397},"outputId":"0131b879-6e9b-4b0c-bb49-00104e84aff5","executionInfo":{"status":"ok","timestamp":1578754751944,"user_tz":-60,"elapsed":5530,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["%tensorflow_version 1.14\n","!pip install keras==2.1.5\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import merge, Input\n","from keras.models import Model\n","from keras.layers.core import Dense, Dropout, Flatten\n","from keras.layers.convolutional import MaxPooling2D, Convolution2D\n","\n","img_rows, img_cols = 150, 150\n","\n","nb_filters_1 = 32\n","nb_filters_2 = 64\n","nb_filters_3 = 128\n","nb_filters_4 = 192\n","nb_conv = 3\n","\n","init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","merge1 = keras.layers.Subtract()([fork11, fork12,])\n","maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool1)\n","fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","merge3 = keras.layers.Subtract()([fork31, fork32, ]) \n","maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool3)\n","fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool3)\n","merge4 = keras.layers.Subtract()([fork41, fork42, ]) \n","maxpool4 = keras.layers.MaxPooling2D(strides=(2,2), )(merge4)\n","\n","\n","\n","flatten = keras.layers.Flatten()(maxpool4)\n","dense1 = keras.layers.Dense(192, activation=\"relu\")(flatten)\n","dropout = keras.layers.Dropout(0.4)(dense1)\n","dense2 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","dense3 = keras.layers.Dense(64, activation=\"relu\")(dense2)\n","output = keras.layers.Dense(4, activation=\"softmax\")(dense3)\n","\n","\n","DCCNN_MODEL = keras.models.Model((init1, init2), output)\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n","You set: `1.14`. This will be interpreted as: `1.x`.\n","\n","\n","TensorFlow is already loaded. Please restart the runtime to change versions.\n","Collecting keras==2.1.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n","\u001b[K     |████████████████████████████████| 337kB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras==2.1.5) (1.18.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras==2.1.5) (1.13.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.5) (1.4.1)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.1.5\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xfQpepw0PSYf","colab":{}},"source":["DCCNN_MODEL.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eX0yKHROPSYj","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"esnacdzCPSYn","colab":{}},"source":["DCCNN_MODEL.compile(optimizer='Adam',\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-d0bU5suQEHT","colab_type":"text"},"source":["## Data Manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NxdbCwbbqM5z","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","\n","\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","\n","\n","train_images_Augmented = train_images_Augmented.astype('float32') / 65535\n","\n","\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GjjweISPqANP","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NnQYpjHUqANT","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hf7wns4gXJoQ","colab_type":"code","colab":{}},"source":["\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"73vtdb_ZPSYq"},"source":["## 10-Cross validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HuKTVao-PSYr","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_model():\n","\n","  nb_filters_1 = 32\n","  nb_filters_2 = 64\n","  nb_filters_3 = 128\n","  nb_filters_4 = 192\n","  nb_conv = 3\n","\n","  init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","  init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","  fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","  fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","  merge1 = keras.layers.Subtract()([fork11, fork12,])\n","  maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","  fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool1)\n","  fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","  merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","  maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","  fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","  fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","  merge3 = keras.layers.Subtract()([fork31, fork32, ]) \n","  maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","  fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool3)\n","  fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool3)\n","  merge4 = keras.layers.Subtract()([fork41, fork42, ]) \n","  maxpool4 = keras.layers.MaxPooling2D(strides=(2,2), )(merge4)\n","\n","\n","\n","  flatten = keras.layers.Flatten()(maxpool4)\n","  dense1 = keras.layers.Dense(192, activation=\"relu\")(flatten)\n","  dropout = keras.layers.Dropout(0.5)(dense1)\n","  dense2 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","  dense3 = keras.layers.Dense(64, activation=\"relu\")(dense2)\n","  output = keras.layers.Dense(4, activation=\"softmax\")(dense3)\n","  output = keras.layers.Dense(4, activation=\"softmax\")(dense3)\n","\n","  Cross_model = keras.models.Model((init1, init2), output)\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['categorical_accuracy']\n","  val_acc = history.history['val_categorical_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()\n","  return\n","\n","\n","############################# CROSS VALIDATION ######################\n","\n","k = 10\n","\n","num_epochs = 40\n","History_Arr =[]\n","Cross_model = get_model()\n","Cross_model.compile(optimizer='RMSprop',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","\n","\n","num_val_samples = len(baseline) // k\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data_ABNORMALITY =  abnormality[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets_ABNORMALITY = abnormality_target_Categorical[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_data_BASELINE = baseline[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data_ABN =    np.concatenate([abnormality[:i * num_val_samples],abnormality[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets_ABN = np.concatenate([abnormality_target_Categorical[:i * num_val_samples],abnormality_target_Categorical[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_data_BASE = np.concatenate([baseline[:i * num_val_samples],baseline[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit([partial_train_data_BASE,partial_train_data_ABN ], partial_train_targets_ABN, validation_data=((val_data_BASELINE,val_data_ABNORMALITY), val_targets_ABNORMALITY), epochs=num_epochs, batch_size=32)\n","  History_Arr.append(history)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer='RMSprop',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-_iLlsujPSYu"},"source":["## FITTING COMPLETO\n","Dopo la cross validation, fitto su tutto il DATASET"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C259C7E2PSYv","colab":{}},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n","\n","num_epochs = 40\n","history = DCCNN_MODEL.fit([baseline,abnormality ], abnormality_target_Categorical ,   epochs=num_epochs, batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UWZl3MlvPSYy"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9_GEHCKwPSYz","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled,Test_abnormality_Not_Shuffled), Test_abnormality_target_Categorical,batch_size=32, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mxsNpr2zPSY2"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"id":"cb2ipJ5PFleB","colab_type":"code","colab":{}},"source":["model1 = tf.keras.models.load_model(os.path.join(base_dir,'Models/Model_Task_4_DCCN_BaseVersion_0.55.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"amvAHPyqoQG7"},"source":["## Adding Data Augmentation "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"A1Of1tSUoQG-"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EuHIUBadoQG_","colab":{}},"source":["def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SFtJ9qH7oQHE"},"source":["Test manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mhVt3ZyJoQHG","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5aae2a4d-abba-4a6f-a1c5-1f34cb037476","executionInfo":{"status":"ok","timestamp":1578754325896,"user_tz":-60,"elapsed":1148,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z17Nrj7poQHK","colab":{}},"source":["print(Test_abnormality_Not_Shuffled)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9P4VgXZXoQHN","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"dea10a4b-24d1-48be-c3c4-9e8d77950cb5","executionInfo":{"status":"ok","timestamp":1578754329664,"user_tz":-60,"elapsed":850,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[[0. 1. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 1. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 1. 0.]\n"," [0. 0. 1. 0.]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Yd9FrvtoQHS","colab":{}},"source":["#ABNORMALITY OPERATIONS\n","\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FM_OS_pvoQHW","colab":{}},"source":["print(base_train_data)\n","print(base_train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_bYUomHjoQHY","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def double_generator(train_base_images, train_abn_images, train_labels, subset, batch_size=32):\n","\n","    gen = ImageDataGenerator(\n","      validation_split=0.1,\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.2, \n","      horizontal_flip=True,\n","      vertical_flip = True,\n","      fill_mode='nearest'\n","    )\n","\n","    gen.fit(train_abn_images)\n","\n","    gen_abn = gen.flow(train_abn_images, train_labels,  batch_size=batch_size, subset=subset, seed=1)\n","    gen_base = gen.flow(train_base_images, train_labels, batch_size=batch_size, subset=subset, seed=1)\n","\n","    while True:\n","        abn_img, abn_label = gen_abn.next()\n","        base_img, _ = gen_base.next()\n","        yield [base_img, abn_img], abn_label\n","\n","train_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'training', batch_size=32)\n","validation_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'validation', batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BwyH_DDNoQHa"},"source":["### FITTING"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9J9SyK6QoQHb","colab":{}},"source":["history = DCCNN_MODEL.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=50,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0hIdaR4poQHd"},"source":["### Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5D_hyBSooQHd","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"33158b68-7635-469c-f4ef-bba0d2350b74","executionInfo":{"status":"ok","timestamp":1578757912833,"user_tz":-60,"elapsed":850,"user":{"displayName":"Davide Rasla","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAMfI4Zr2jEYq07Vr2MwFZyeSUvYDePIfpSq0Duo-s=s64","userId":"04903060551443189044"}}},"source":["\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled, Test_abnormality_Not_Shuffled),Test_abnormality_target_Categorical,batch_size=32,  verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["336/336 [==============================] - 0s 431us/sample - loss: 0.8652 - categorical_accuracy: 0.6577\n","0.8652134906677973\n","0.6577381\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jcv3cemSoQHf"},"source":["### Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UC9XS573oQHg","colab":{}},"source":["#Save the model!\n","DCCNN_MODEL.save(os.path.join(base_dir,'Models/Model_Task_4_DCCNN_Corretto_WithDataAugmentation_0.6578.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3NSHifh9Uz6S"},"source":["# USING DCCNN with baseline for binary classification (Deep - Columnar)\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0K5qp1TkUz6V"},"source":["## Model "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zipLRoMwUz6W","colab":{}},"source":["%tensorflow_version 1.14\n","!pip install keras==2.1.5\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import merge, Input\n","from keras.models import Model\n","from keras.layers.core import Dense, Dropout, Flatten\n","from keras.layers.convolutional import MaxPooling2D, Convolution2D\n","\n","img_rows, img_cols = 150, 150\n","\n","nb_filters_1 = 32\n","nb_filters_2 = 64\n","nb_filters_3 = 128\n","nb_filters_4 = 192\n","nb_conv = 3\n","\n","init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","merge1 = keras.layers.Subtract()([fork11, fork12,])\n","maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool1)\n","fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","merge3 = keras.layers.Subtract()([fork31, fork32, ]) \n","maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool3)\n","fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool3)\n","merge4 = keras.layers.Subtract()([fork41, fork42, ]) \n","maxpool4 = keras.layers.MaxPooling2D(strides=(2,2), )(merge4)\n","\n","\n","\n","flatten = keras.layers.Flatten()(maxpool4)\n","dense1 = keras.layers.Dense(192, activation=\"relu\")(flatten)\n","dropout = keras.layers.Dropout(0.5)(dense1)\n","dense2 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","dense3 = keras.layers.Dense(64, activation=\"relu\")(dense2)\n","output = keras.layers.Dense(2, activation=\"softmax\")(dense3)\n","\n","\n","DCCNN_MODEL = keras.models.Model((init1, init2), output)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_OuQL8jKUz6c","colab":{}},"source":["DCCNN_MODEL.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CvZatJ15Uz6g","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mJ_1xVoMUz6i","colab":{}},"source":["DCCNN_MODEL.compile(optimizer='Adam',\n","            loss='binary_crossentropy', metrics=['binary_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IwZfmxeKUz6k"},"source":["## Data Manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zy6f7kY1Uz6l","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","train_images_Augmented = train_images_Augmented.astype('float32') / 65535\n","\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h-sEDV9NUz6m","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 1\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,2)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dVxZsvsiUz6q","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 1\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,2)\n","baseline_target_Categorical = to_categorical(baseline_target,2)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G3gIHGSZUz6t","colab":{}},"source":["\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YjgXyhdMUz6v"},"source":["## 10-Cross validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lMO6YugFUz6w","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_model():\n","\n","  img_rows, img_cols = 150, 150\n","\n","  nb_filters_1 = 64\n","  nb_filters_2 = 129\n","  nb_filters_3 = 256\n","  nb_filters_4 = 192\n","  nb_conv = 3\n","\n","  init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","  init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","  fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","  fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","  merge1 = keras.layers.Subtract()([fork11, fork12,])\n","  maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","  fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool1)\n","  fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","  merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","  maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","  fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","  fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation=\"relu\",  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool2)\n","  merge3 = keras.layers.Subtract()([fork31, fork32, ]) \n","  maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","  fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool3)\n","  fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation=\"relu\", kernel_regularizer=keras.regularizers.l2(0.001))(maxpool3)\n","  merge4 = keras.layers.Subtract()([fork41, fork42, ]) \n","  maxpool4 = keras.layers.MaxPooling2D(strides=(2,2), )(merge4)\n","\n","\n","\n","  flatten = keras.layers.Flatten()(maxpool4)\n","  dense1 = keras.layers.Dense(192, activation=\"relu\")(flatten)\n","  dropout = keras.layers.Dropout(0.5)(dense1)\n","  dense2 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","  dense3 = keras.layers.Dense(64, activation=\"relu\")(dense2)\n","  output = keras.layers.Dense(2, activation=\"softmax\")(dense3)\n","\n","  Cross_model = keras.models.Model((init1, init2), output)\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['binary_crossentropy']\n","  val_acc = history.history['val_binary_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()\n","  return\n","\n","\n","############################# CROSS VALIDATION ######################\n","\n","k = 10\n","\n","num_epochs = 40\n","History_Arr =[]\n","Cross_model = get_model()\n","Cross_model.compile(optimizer='RMSprop',loss='binary_crossentropy', metrics=['binary_accuracy'])\n","\n","print(abnormality_target_Categorical)\n","num_val_samples = len(baseline) // k\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data_ABNORMALITY =  abnormality[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets_ABNORMALITY = abnormality_target_Categorical[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_data_BASELINE = baseline[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data_ABN =    np.concatenate([abnormality[:i * num_val_samples],abnormality[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets_ABN = np.concatenate([abnormality_target_Categorical[:i * num_val_samples],abnormality_target_Categorical[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_data_BASE = np.concatenate([baseline[:i * num_val_samples],baseline[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit([partial_train_data_BASE,partial_train_data_ABN ], partial_train_targets_ABN, validation_data=((val_data_BASELINE,val_data_ABNORMALITY), val_targets_ABNORMALITY), epochs=num_epochs, batch_size=32)\n","  History_Arr.append(history)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer='RMSprop',loss='binary_crossentropy', metrics=['binary_accuracy'])\n","  plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPqFy4gw-4Ub","colab_type":"code","colab":{}},"source":["def plot():\n","  acc = history.history['binary_accuracy']\n","  val_acc = history.history['val_binary_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()\n","  return\n","plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DQTTPsZuUz6y"},"source":["## FITTING COMPLETO"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MLQPoA24Uz6y","colab":{}},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n","\n","num_epochs = 40\n","history = DCCNN_MODEL.fit([baseline,abnormality ], abnormality_target_Categorical ,   epochs=num_epochs, batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iLYGcSJqUz62"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o4hhbXIqUz63","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled,Test_abnormality_Not_Shuffled), Test_abnormality_target_Categorical,batch_size=32, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OYBaA5M4Uz65"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N1c11kQ5Uz65","colab":{}},"source":["#Save the model!\n","DCCNN_MODEL.save(os.path.join(base_dir,'Models/Model_Task_2.2_DCCN_Baseline_BaseVersion_0.86.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"g_gaxOcPUz68"},"source":["## Adding Data Augmentation "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"o77FT242Uz69"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H1r0abnwUz6-","colab":{}},"source":["def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GMYcJ2eoUz7B"},"source":["Test manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y4sS3eAjUz7C","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 1\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,2)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c39stt8mUz7E","colab":{}},"source":["print(Test_abnormality_Not_Shuffled)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hsenjPJ3Uz7G","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 1\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,2)\n","baseline_target_Categorical = to_categorical(baseline_target,2)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MlTN0pKGUz7I","colab":{}},"source":["#ABNORMALITY OPERATIONS\n","\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zqnbwzqBUz7O","colab":{}},"source":["print(base_train_data)\n","print(base_train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IMt9wggoUz7R","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def double_generator(train_base_images, train_abn_images, train_labels, subset, batch_size=32):\n","\n","    gen = ImageDataGenerator(\n","      validation_split=0.1,\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.1, \n","      horizontal_flip=True,\n","      vertical_flip = True,\n","      fill_mode='nearest'\n","    )\n","\n","    gen.fit(train_abn_images)\n","\n","    gen_abn = gen.flow(train_abn_images, train_labels,  batch_size=batch_size, subset=subset, seed=1)\n","    gen_base = gen.flow(train_base_images, train_labels, batch_size=batch_size, subset=subset, seed=1)\n","\n","    while True:\n","        abn_img, abn_label = gen_abn.next()\n","        base_img, _ = gen_base.next()\n","        yield [base_img, abn_img], abn_label\n","\n","train_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'training', batch_size=32)\n","validation_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'validation', batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WZZMGiJXUz7T"},"source":["### FITTING"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qvXqX0U6Uz7U","colab":{}},"source":["history = DCCNN_MODEL.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=2,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FiTan2-fUz7W"},"source":["### Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NyjZ49P2Uz7X","colab":{}},"source":["\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled, Test_abnormality_Not_Shuffled),Test_abnormality_target_Categorical,batch_size=32,  verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0V1Jp8JKUz7a"},"source":["### Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Nc3ey__WUz7b","colab":{}},"source":["#Save the model!\n","DCCNN_MODEL.save(os.path.join(base_dir,'Models/Model_Task_4_DCCNN_Binary_WithDataAugmentation_0.8869.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cojczrmazwej"},"source":["# USING dual-channel classification\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5610YsIczwem"},"source":["## Model "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bcyzdhM3zwen","colab":{}},"source":["%tensorflow_version 1.14\n","!pip install keras==2.1.5\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import merge, Input\n","from keras.models import Model\n","from keras.layers.core import Dense, Dropout, Flatten\n","from keras.layers.convolutional import MaxPooling2D, Convolution2D\n","\n","img_rows, img_cols = 150, 150\n","nb_filters_1 = 32\n","nb_filters_2 = 64\n","nb_filters_3 = 128\n","nb_filters_4 = 256\n","nb_conv = 3\n","\n","\n","init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1), input_shape=(None, 150, 150, 1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1), input_shape=(None, 150, 150, 1), activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","maxpool11 = keras.layers.MaxPooling2D(strides=(2,2),)(fork11)\n","maxpool22 = keras.layers.MaxPooling2D(strides=(2,2),)(fork12)\n","\n","fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool11)\n","fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(maxpool22)\n","\n","maxpool21 = keras.layers.MaxPooling2D(strides=(2,2), )(fork21)\n","maxpool22 = keras.layers.MaxPooling2D(strides=(2,2), )(fork22)\n","fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation='relu',  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool21)\n","fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation='relu',  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool22)\n","\n","maxpool31 = keras.layers.MaxPooling2D(strides=(2,2), )(fork31)\n","maxpool32 = keras.layers.MaxPooling2D(strides=(2,2), )(fork32)\n","fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool31)\n","fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(maxpool32)\n","merge4 = keras.layers.Subtract()([fork21, fork22, ]) #One single merge(Channel1, Channel 2)\n","flatten = keras.layers.Flatten()(merge4)\n","dense1 = keras.layers.Dense(256, activation=\"relu\")(flatten)\n","dropout1 = keras.layers.Dropout(0.4)(dense1)\n","dense3 = keras.layers.Dense(128, activation=\"relu\")(dropout1)\n","output = keras.layers.Dense(4, activation=\"softmax\")(dense3)\n","\n","\n","\n","DCCNN_MODEL = keras.models.Model((init1, init2), output)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KTYfB4e4zwer","colab":{}},"source":["DCCNN_MODEL.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4wMKYRdvzwet","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3hwEFANLzwev","colab":{}},"source":["DCCNN_MODEL.compile(optimizer='Adam',\n","              loss='categorical_crossentropy',\n","              metrics=['categorical_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"swZ70SNEzwew"},"source":["## Data Manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60BHqfXuzwex","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","\n","\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","\n","train_images_Augmented = train_images_Augmented.astype('float32') / 65535\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wEsmc-yYzwe0","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F_9EOwpBzwe1","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2wGkHc8Uzwe3","colab":{}},"source":["\n","print(abnormality_target_Categorical)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"StCEXj0Czwe5"},"source":["## 10-Cross validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LoSJJKbezwe6","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_model():\n","\n","  img_rows, img_cols = 150, 150\n","  img_rows, img_cols = 150, 150\n","  nb_filters_1 = 32\n","  nb_filters_2 = 64\n","  nb_filters_3 = 128\n","  nb_filters_4 = 256\n","  nb_conv = 3\n","\n","\n","  init1 = keras.layers.Input(shape=(150,150,1),) #channel 1\n","\n","\n","  init2 = keras.layers.Input(shape=(150,150,1),) #channel 2\n","\n","  fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), strides=(1,1), input_shape=(None, 150, 150, 1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(init1)\n","  fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),strides=(1,1), input_shape=(None, 150, 150, 1), activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(init2)\n","  maxpool11 = keras.layers.MaxPooling2D(strides=(2,2),)(fork11)\n","  maxpool22 = keras.layers.MaxPooling2D(strides=(2,2),)(fork12)\n","\n","  fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool11)\n","  fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(maxpool22)\n","\n","  maxpool21 = keras.layers.MaxPooling2D(strides=(2,2), )(fork21)\n","  maxpool22 = keras.layers.MaxPooling2D(strides=(2,2), )(fork22)\n","  fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation='relu',  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool21)\n","  fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv),strides=(1,1),  activation='relu',  kernel_regularizer=keras.regularizers.l2(0.001))(maxpool22)\n","\n","  maxpool31 = keras.layers.MaxPooling2D(strides=(2,2), )(fork31)\n","  maxpool32 = keras.layers.MaxPooling2D(strides=(2,2), )(fork32)\n","  fork41 = keras.layers.Conv2D(nb_filters_4, (3, 3),strides=(1,1),  activation='relu', kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool31)\n","  fork42 = keras.layers.Conv2D(nb_filters_4, (3, 3), strides=(1,1), activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))(maxpool32)\n","  merge4 = keras.layers.Subtract()([fork21, fork22, ]) #One single merge(Channel1, Channel 2)\n","  flatten = keras.layers.Flatten()(merge4)\n","  dense1 = keras.layers.Dense(256, activation=\"relu\")(flatten)\n","  dropout1 = keras.layers.Dropout(0.4)(dense1)\n","  dense3 = keras.layers.Dense(128, activation=\"relu\")(dropout1)\n","  output = keras.layers.Dense(4, activation=\"softmax\")(dense3)\n","\n","\n","  Cross_model = keras.models.Model((init1, init2), output)\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['categorical_accuracy']\n","  val_acc = history.history['val_categorical_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.show()\n","  return\n","\n","\n","############################# CROSS VALIDATION ######################\n","\n","k = 10\n","\n","num_epochs = 70\n","History_Arr =[]\n","Cross_model = get_model()\n","Cross_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","\n","num_val_samples = len(baseline_Not_Shuffled) // k\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data_ABNORMALITY =  abnormality[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets_ABNORMALITY = abnormality_target_Categorical[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_data_BASELINE = baseline[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data_ABN =    np.concatenate([abnormality[:i * num_val_samples],abnormality[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets_ABN = np.concatenate([abnormality_target_Categorical[:i * num_val_samples],abnormality_target_Categorical[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_data_BASE = np.concatenate([baseline[:i * num_val_samples],baseline[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit([partial_train_data_BASE,partial_train_data_ABN ], partial_train_targets_ABN, validation_data=((val_data_BASELINE,val_data_ABNORMALITY), val_targets_ABNORMALITY), epochs=num_epochs, batch_size=32)\n","  History_Arr.append(history)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer='Adam',loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","  plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SSlToiI-zwe8"},"source":["## FITTING COMPLETO\n","Dopo la cross validation, fitto su tutto il DATASET"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W_G6AGanzwe8","colab":{}},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n","\n","num_epochs = 60\n","history = DCCNN_MODEL.fit([baseline,abnormality ], abnormality_target_Categorical , callbacks=[callback],  epochs=num_epochs, batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"reM9AdwEzwe-"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xcvvvNpqzwe_","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled,Test_abnormality_Not_Shuffled), Test_abnormality_target_Categorical,batch_size=32, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PTbYuqI-zwfB"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1LrgZSDczwfB","colab":{}},"source":["#Save the model!\n","DCCNN_MODEL.save(os.path.join(base_dir,'Models/Model_Task_4_Dual-Channel_BaseVersion_0.52.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MKCrKDRozwfD"},"source":["## Adding Data Augmentation "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eXyEBjoBzwfD"},"source":["### Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JmBz2sMqzwfE","colab":{}},"source":["def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6VhcEj77zwfG"},"source":["Test manipulation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xaXsPulLzwfH","colab":{}},"source":["#TEST\n","\n","from keras.utils import to_categorical\n","#splitting\n","Test_baseline_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_baseline_Not_Shuffled =  test_images_Augmented[::2]\n","\n","Test_baseline_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_baseline_target_Not_Shuffled =  test_lable_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","Test_abnormality_Not_Shuffled =  np.empty_like(test_images_Augmented)\n","Test_abnormality_Not_Shuffled =   test_images_Augmented[1:][::2]\n","\n","Test_abnormality_target_Not_Shuffled =  np.empty_like(test_lable_Augmented)\n","Test_abnormality_target_Not_Shuffled =   test_lable_Augmented[1:][::2] #ora contiene solo le label delle abnormality\n","\n","\n","#editing labels\n","leng = len(Test_abnormality_target_Not_Shuffled)\n","Test_abnormality_target_4 = np.empty_like(Test_abnormality_target_Not_Shuffled)\n","for i in range(leng):\n","    if Test_abnormality_target_Not_Shuffled[i] == 1:\n","      Test_abnormality_target_4[i] = 0\n","    if Test_abnormality_target_Not_Shuffled[i] == 2:\n","      Test_abnormality_target_4[i] = 1\n","    if Test_abnormality_target_Not_Shuffled[i] == 3:\n","      Test_abnormality_target_4[i] = 2\n","    if Test_abnormality_target_Not_Shuffled[i] == 4:\n","      Test_abnormality_target_4[i] = 3\n","\n","\n","#TO_CATEGORICAL\n","Test_abnormality_target_Categorical = to_categorical(Test_abnormality_target_4,4)\n","Test_baseline_target_Categorical = to_categorical(Test_baseline_target_Not_Shuffled,4)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PkAlj5QYzwfJ","colab":{}},"source":["print(Test_abnormality_Not_Shuffled)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xDlka7F3zwfK","colab":{}},"source":["import numpy as np\n","from keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","\n","\n","#splitting\n","baseline_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","baseline_Not_Shuffled =  train_images_Augmented[::2]\n","\n","baseline_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","baseline_target_Not_Shuffled =  train_labels_Augmented[::2]\n","\n","#ABNORMALITY OPERATIONS\n","abnormality_Not_Shuffled =  np.empty_like(train_images_Augmented)\n","abnormality_Not_Shuffled =   train_images_Augmented[1:][::2]\n","\n","abnormality_target_Not_Shuffled =  np.empty_like(train_labels_Augmented)\n","abnormality_target_Not_Shuffled =   train_labels_Augmented[1:][::2] #ora contiene solo le label delle abnormaluity\n","\n","#shuffling\n","\n","abnormality, abnormality_target = shuffle( abnormality_Not_Shuffled, abnormality_target_Not_Shuffled, random_state=42)\n","baseline, baseline_target = shuffle( baseline_Not_Shuffled, baseline_target_Not_Shuffled, random_state=42)\n","\n","#editing labels\n","leng = len(abnormality_target)\n","abnormality_target_4 = np.empty_like(abnormality_target)\n","for i in range(leng):\n","    if abnormality_target[i] == 1:\n","      abnormality_target_4[i] = 0\n","    if abnormality_target[i] == 2:\n","      abnormality_target_4[i] = 1\n","    if abnormality_target[i] == 3:\n","      abnormality_target_4[i] = 2\n","    if abnormality_target[i] == 4:\n","      abnormality_target_4[i] = 3\n","\n","#categorical\n","\n","abnormality_target_Categorical = to_categorical(abnormality_target_4,4)\n","baseline_target_Categorical = to_categorical(baseline_target,4)\n","\n","\n","\n","print(abnormality_target_Categorical[:10])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2rJNRF5GzwfP","colab":{}},"source":["#ABNORMALITY OPERATIONS\n","\n","print(abnormality_target)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZfhhfYzWzwfS","colab":{}},"source":["print(base_train_data)\n","print(base_train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TIHRNmcazwfU","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def double_generator(train_base_images, train_abn_images, train_labels, subset, batch_size=32):\n","\n","    gen = ImageDataGenerator(\n","      validation_split=0.1,\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.1, \n","      horizontal_flip=True,\n","      vertical_flip = True,\n","      fill_mode='nearest'\n","    )\n","\n","    gen.fit(train_abn_images)\n","\n","    gen_abn = gen.flow(train_abn_images, train_labels,  batch_size=batch_size, subset=subset, seed=1)\n","    gen_base = gen.flow(train_base_images, train_labels, batch_size=batch_size, subset=subset, seed=1)\n","\n","    while True:\n","        abn_img, abn_label = gen_abn.next()\n","        base_img, _ = gen_base.next()\n","        yield [base_img, abn_img], abn_label\n","\n","train_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'training', batch_size=32)\n","validation_generator = double_generator(baseline, abnormality, abnormality_target_Categorical, 'validation', batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"G4r8_IbrzwfV"},"source":["### FITTING"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zJHllmChzwfW","colab":{}},"source":["history = DCCNN_MODEL.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=1,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RMhVNPvizwfY"},"source":["### Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RzVvmKl6zwfY","colab":{}},"source":["\n","\n","test_loss, test_acc = DCCNN_MODEL.evaluate((Test_baseline_Not_Shuffled, Test_abnormality_Not_Shuffled),Test_abnormality_target_Categorical,batch_size=32,  verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PRh2FVuXzwfa"},"source":["### Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8UPRaAurzwfb","colab":{}},"source":["#Save the model!\n","DCCNN_MODEL.save(os.path.join(base_dir,'Models/Model_Task_4_DualChannel_Corretto_WithDataAugmentation_0.5982.h5'))"],"execution_count":0,"outputs":[]}]}
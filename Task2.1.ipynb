{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Task2.1.ipynb","provenance":[{"file_id":"1Kg73xvECiXFiiZBra5n-zdPjh3jwn6h9","timestamp":1575496164434}],"collapsed_sections":["Y3oU4oSF4JAS","Iq1-POSf7zHs","57CXX2i974iQ","B5JJLwdUmUa-","HXdI32eb8D6L","PsGZ703x79lN","SeC7FAlw8Iun","IljNnNoexBiz","FWpenDk88cV4","n6YJIftF8Q-I","i9tnjTD5D1wG","0oytq70j9i12","UqX1Arww8hDr","qr5eJHFhvgmk","JNWNIQUN8noI","DHDBPvLfD8eZ","eWD7U12V8rul","aq94s8Ik9l3I","x6fjqgl889c6","qKIQ1p9L4Ror","N1AASGgD9BCb","yLd2c8T49EU8","d_LOyCTO52sR","0iHVH8MW-Myl","ThVHnqCTA37-","S_R8ypMfBLY8","DtBEAO3aOIhq","zbZGIDq_OIh5","ozh6CO8aOIh_","vLLmznjNJlIq","sg7jl7PcMj2y","4l9wYp7Q0ljK","MndqbMWIlMit","IO_TcvZSnXyJ","16_fHIRB6r0m","NRBaz71Z90C7","MAMzmjG-Ko8G","-Xyab9KVKo8S"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Y3oU4oSF4JAS","colab_type":"text"},"source":["# Task 2.1\n","Development of a classification model for discriminating between 2 classes: masses and calcification. Design and development of an ad-hoc CNN architecture  (training from scratch)"]},{"cell_type":"code","metadata":{"id":"NKg9omoXwnfO","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","print(tf.__version__)\n","from tensorflow import keras\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATgiy_z6B0IQ","colab_type":"code","colab":{}},"source":["import os\n","from google.colab import drive\n","import numpy as np\n","drive.mount('/content/drive')\n","os.listdir()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zg-pz2q7tnNd","colab_type":"code","colab":{}},"source":["base_dir = 'drive/My Drive/Computational Intelligence - MY PROJECT/My_Project_CompInt' \n","os.listdir(base_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aC10QEDb5qq","colab_type":"code","colab":{}},"source":["#Restore the saved model\n","model = tf.keras.models.load_model(os.path.join(base_dir,'Models/Model_Task2.1_BaseVersion.h5'))\n","\n","# Show the model architecture\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iq1-POSf7zHs","colab_type":"text"},"source":["# Data Manipulation"]},{"cell_type":"code","metadata":{"id":"EnKD_z8M4B7T","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images, train_labels, test_images, test_lable = load_training()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57CXX2i974iQ","colab_type":"text"},"source":["## Reshape"]},{"cell_type":"code","metadata":{"id":"1NjntIxS76yX","colab_type":"code","colab":{}},"source":["#reshape per poter fittare la CNN\n","train_images = train_images.reshape((5352, 150, 150,1))\n","train_images = train_images.astype('float32') / 65535\n","test_images = test_images.reshape((672, 150, 150,1))\n","test_images = test_images.astype('float32') / 65535\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_wjd_hjoHab","colab_type":"code","colab":{}},"source":["print(train_images[0:100])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5JJLwdUmUa-","colab_type":"text"},"source":["#DATASet manipulation"]},{"cell_type":"markdown","metadata":{"id":"HXdI32eb8D6L","colab_type":"text"},"source":["## Deleting the baseline"]},{"cell_type":"code","metadata":{"id":"7BEqVKRRNsi-","colab_type":"code","colab":{}},"source":["#Here i manage the input tensors, removing the baseline patch and the relative labels in order to perform binary classification\n","SubTrainArray =  np.empty_like(train_images)\n","SubTrainArray =  np.delete(train_images, np.s_[::2], 0)\n","print(SubTrainArray[1])\n","print(SubTrainArray[2])\n","print(SubTrainArray[3])\n","SubLabelArray = np.empty_like(train_labels)\n","SubLabelArray =  np.delete(train_labels, np.s_[::2], 0)\n","print(SubLabelArray[0:100])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrgRHC6Hobl8","colab_type":"code","colab":{}},"source":["print(SubLabelArray[0:1000])\n","print(SubLabelArray[1000:2000])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsGZ703x79lN","colab_type":"text"},"source":["## Editing the label "]},{"cell_type":"code","metadata":{"id":"cdj6FWFypGqz","colab_type":"code","colab":{}},"source":["#Here i manage the input tensor and the label in order to perform binary classification\n","\n","leng = len(SubLabelArray)\n","SubLabelArray_Binary = np.empty_like(SubLabelArray)\n","for i in range(leng):\n","    if SubLabelArray[i] == 1 or SubLabelArray[i] == 2 :\n","      SubLabelArray_Binary[i] = 0\n","    if SubLabelArray[i] == 3 or SubLabelArray[i] == 4:\n","      SubLabelArray_Binary[i] = 1\n","\n","print(SubLabelArray_Binary[0:1000])\n","print(SubLabelArray_Binary[2000:])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SeC7FAlw8Iun","colab_type":"text"},"source":["## Shuffle"]},{"cell_type":"code","metadata":{"id":"VzPc9RK2I340","colab_type":"code","colab":{}},"source":["#eseguo uno shuffle dei dati \n"," from sklearn.utils import shuffle\n","\n"," SubTrainArray_Shuffled, SubLabelArray_Shuffled = shuffle( SubTrainArray, SubLabelArray_Binary, random_state=42)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1DiFYtIrZmuP","colab_type":"code","colab":{}},"source":["#Printing in order to check if the shuffled has been performed well\n","print(SubLabelArray_Shuffled[0:1000])\n","print(SubLabelArray_Shuffled[2000:3000])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IljNnNoexBiz","colab_type":"text"},"source":["## Modifiyng the test set"]},{"cell_type":"markdown","metadata":{"id":"FWpenDk88cV4","colab_type":"text"},"source":["### Deleting BaseLine"]},{"cell_type":"code","metadata":{"id":"cx9VT-ywsA2h","colab_type":"code","colab":{}},"source":["#Here i manage the TEST data set, removing baseline\n","SubTestArray =  np.empty_like(test_images)\n","SubTestArray =  np.delete(test_images, np.s_[::2], 0)\n","\n","SubTestLabelArray = np.empty_like(test_lable)\n","SubTestLabelArray =  np.delete(test_lable, np.s_[::2], 0)\n","print(SubTestLabelArray[0:1000])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6YJIftF8Q-I","colab_type":"text"},"source":["### Editing the labels "]},{"cell_type":"code","metadata":{"id":"AVfExRScsFob","colab_type":"code","colab":{}},"source":["#Here i modify the label\n","SubTestLabelArray_Edited = np.zeros_like(SubTestLabelArray)\n","\n","leng = len(SubTestLabelArray)\n","for i in range(leng):\n","    if SubTestLabelArray[i] == 1 or SubTestLabelArray[i] == 2 :\n","      SubTestLabelArray_Edited[i] = 0\n","    if SubTestLabelArray[i] == 3 or SubTestLabelArray[i] == 4:\n","      SubTestLabelArray_Edited[i] = 1\n","\n","print(SubTestLabelArray_Edited[0:1000])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9tnjTD5D1wG","colab_type":"text"},"source":["# Building The Network: Base version"]},{"cell_type":"markdown","metadata":{"id":"0oytq70j9i12","colab_type":"text"},"source":["## Define the model"]},{"cell_type":"code","metadata":{"id":"xwB5lZ5kyRTX","colab_type":"code","colab":{}},"source":["keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from keras.layers.convolutional_recurrent import ConvLSTM2D\n","\n","model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 1)))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu',))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(128, (3, 3), activation='relu',))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(192, (3, 3), activation='relu',))\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Flatten())\n","model.add(layers.Dense(192, activation='relu'))\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(32, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ERd7f4BwDJX","colab_type":"code","colab":{}},"source":["model.summary()#a summary of the architecture"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2kAkk5ypDfH","colab_type":"code","colab":{}},"source":["model.compile(optimizer='Adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPGew-peAL8M","colab_type":"code","colab":{}},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='binary_accuracy', patience=5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UqX1Arww8hDr","colab_type":"text"},"source":["## Cross-Validation"]},{"cell_type":"code","metadata":{"id":"dde4zFyMmuNP","colab_type":"code","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","\n","def get_model():\n","  Cross_model = models.Sequential()\n","  Cross_model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 1)))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(64, (3, 3), activation='relu',))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(128, (3, 3), activation='relu',))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(128, (3, 3), activation='relu',))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Flatten())\n","  Cross_model.add(layers.Dense(128, activation='relu'))\n","  Cross_model.add(layers.Dense(64, activation='relu'))\n","  Cross_model.add(layers.Dense(32, activation='relu'))\n","  Cross_model.add(layers.Dense(1, activation='sigmoid'))\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['binary_accuracy']\n","  val_acc = history.history['val_binary_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","  epochs = range(len(acc))\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()\n","  return\n","############################# CROSS VALIDATION ######################à\n","k = 10\n","num_val_samples = len(SubTrainArray_Shuffled) // k\n","num_epochs = 40\n","\n","Cross_model = get_model()\n","Cross_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['binary_accuracy'])\n","History_arr=[]\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data = SubTrainArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets = SubLabelArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data = np.concatenate([SubTrainArray_Shuffled[:i * num_val_samples],SubTrainArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets = np.concatenate([SubLabelArray_Shuffled[:i * num_val_samples],SubLabelArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=32)\n","  History_arr.append(history)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['binary_accuracy'])\n","  plot()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrFVtLzyW0AM","colab_type":"code","colab":{}},"source":["print(History_arr[0])\n","import matplotlib.pyplot as plt\n","mean_acc = []\n","mean_val = []\n","mean_loss = []\n","mean_val_loss = []\n","\n","for i in range(len(History_arr)):\n","  acc = History_arr[i].history['binary_accuracy']\n","  val_acc = History_arr[i].history['val_binary_accuracy']\n","  loss = History_arr[i].history['loss']\n","  val_loss = History_arr[i].history['val_loss']\n","  epochs = range(len(acc))\n","  mean_acc += History_arr[i].history['binary_accuracy']\n","  mean_val += History_arr[i].history['val_binary_accuracy']\n","  mean_loss += History_arr[i].history['loss']\n","  mean_val_loss += History_arr[i].history['val_loss']\n","\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","  plt.figure()\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","  plt.savefig(\"figure.png\")\n","  plt.show()\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6ze7T_q34HDK"},"source":["np.mean(hist.history['acc']) # numpy assumed imported as np\n","\n","1.   List item\n","2.   List item\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qr5eJHFhvgmk","colab_type":"text"},"source":["## FITTING COMPLETO\n","Dopo la cross validation, fitto su tutto il DATASET"]},{"cell_type":"code","metadata":{"id":"ttD0naONuNVX","colab_type":"code","colab":{}},"source":[" history = model.fit(SubTrainArray_Shuffled, SubLabelArray_Shuffled, epochs=30,callbacks=[callback], batch_size=32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Js3dtfVzuSS7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNWNIQUN8noI","colab_type":"text"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"id":"mXfwOb3Quzix","colab_type":"code","colab":{}},"source":["#evaluate the model\n","\n","\n","test_loss, test_acc = model.evaluate(SubTestArray, SubTestLabelArray_Edited, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHDBPvLfD8eZ","colab_type":"text"},"source":["### Results on test set"]},{"cell_type":"markdown","metadata":{"id":"9PVSdDrhEAQ2","colab_type":"text"},"source":["336/336 [==============================] - 1s 2ms/sample - loss: 1.0825 - binary_accuracy: 0.7917\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aq94s8Ik9l3I"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-hwI7nut9l3L","colab":{}},"source":["#Restore the saved model\n","model = tf.keras.models.load_model(os.path.join(base_dir,'Models/MY_MODEL_Task_2.1_Base_Definitivo.h5'))\n","\n","# Show the model architecture\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pdgni_8X9l3S","colab":{}},"source":["#Save the model!\n","model.save(os.path.join(base_dir,'Models/MY_MODEL_Task_2.1_Base_Definitivo_0.81.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ja7Pfn6Iu438","colab_type":"text"},"source":["# Building The Network: Adding DROPOUT and weight regularization"]},{"cell_type":"markdown","metadata":{"id":"Ba9dWT4d9doz","colab_type":"text"},"source":["## Define the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UAu8eoX9vd-i","colab":{}},"source":["keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","\n","\n","\n","\n","model_DropOut = models.Sequential()\n","\n","model_DropOut.add(layers.Conv2D(32, (4, 3), activation='relu',input_shape=(150, 150, 1)))\n","model_DropOut.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut.add(layers.Conv2D(192, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut.add(layers.Flatten())\n","model_DropOut.add(layers.Dense(192, activation='relu'))\n","model_DropOut.add(layers.Dropout(0.4))\n","model_DropOut.add(layers.Dense(64, activation='relu'))\n","model_DropOut.add(layers.Dense(1, activation='sigmoid'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HlTxnH1c4Hw3","colab_type":"code","colab":{}},"source":["model_DropOut.summary()#a summary of the architecture"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oWIhIEFkvd-s","colab":{}},"source":["model_DropOut.compile(optimizer=keras.optimizers.RMSprop(lr=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy'])\n","#early stopping\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=10)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6fjqgl889c6","colab_type":"text"},"source":["## CrossValidation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k-cwcTWKvd-u","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","def get_model():\n","  Cross_model = models.Sequential()\n","  Cross_model.add(layers.Conv2D(32, (4, 3), activation='relu',input_shape=(150, 150, 1)))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.0001)))\n","  Cross_model.add(layers.MaxPooling2D((2, 2)))\n","  Cross_model.add(layers.Flatten())\n","  Cross_model.add(layers.Dense(128, activation='relu'))\n","  Cross_model.add(layers.Dropout(0.5))\n","  Cross_model.add(layers.Dense(64, activation='relu'))\n","  Cross_model.add(layers.Dense(1, activation='sigmoid'))\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['binary_accuracy']\n","  val_acc = history.history['val_binary_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","\n","  plt.figure()\n","\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()\n","  return\n","\n","k = 10\n","num_val_samples = len(SubTrainArray_Shuffled) // k\n","num_epochs = 40\n","Cross_model = get_model()\n","Cross_model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001),loss='binary_crossentropy', metrics=['binary_accuracy'])\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data = SubTrainArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets = SubLabelArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data = np.concatenate([SubTrainArray_Shuffled[:i * num_val_samples],SubTrainArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets = np.concatenate([SubLabelArray_Shuffled[:i * num_val_samples],SubLabelArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets), epochs=num_epochs,callbacks=[callback], batch_size=32)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer=keras.optimizers.RMSprop(lr=0.001),loss='binary_crossentropy', metrics=['binary_accuracy'])\n","  plot()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qKIQ1p9L4Ror"},"source":["## FITTING COMPLETO\n","Dopo la cross validation, fitto su tutto il DATASET"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fyam7VhF4Rou","colab":{}},"source":[" history = model_DropOut.fit(SubTrainArray_Shuffled, SubLabelArray_Shuffled, epochs=40,callbacks=[callback], batch_size=32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tfDiwstA4Rox","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1AASGgD9BCb","colab_type":"text"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"koWOZyJqvd-6","colab":{}},"source":["#evaluate the model\n","\n","\n","test_loss, test_acc = model_DropOut.evaluate(SubTestArray, SubTestLabelArray_Edited, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLd2c8T49EU8","colab_type":"text"},"source":["## Plotting"]},{"cell_type":"code","metadata":{"id":"y-2pka86kZQX","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","acc = history.history['binary_accuracy']\n","val_acc = history.history['val_binary_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_LOyCTO52sR","colab_type":"text"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"igOLm6Zk4KVs","colab":{}},"source":["#Save the model!\n","model_DropOut.save(os.path.join(base_dir,'Models/Model_Task_2.1_WithDROPOUT_Definitivo_0.8.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BG-eJO7A9RPc","colab":{}},"source":["#Restore the saved model\n","model_DropOut = tf.keras.models.load_model(os.path.join(base_dir,'Models/Model_Task_2.1_WithDROPOUT_Definitivo_0.8.h5'))\n","\n","# Show the model architecture\n","model_DropOut.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iHVH8MW-Myl","colab_type":"text"},"source":["# Building The Network: Adding Data Aumentation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ThVHnqCTA37-"},"source":["## Define the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CYSPhWTJA38B","colab":{}},"source":["keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","model_DropOut_Augmentation = models.Sequential()\n","\n","model_DropOut_Augmentation.add(layers.Conv2D(32, (4, 3), activation='relu',input_shape=(150, 150, 1)))\n","model_DropOut_Augmentation.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut_Augmentation.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut_Augmentation.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut_Augmentation.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut_Augmentation.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut_Augmentation.add(layers.Conv2D(192, (3, 3), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)))\n","model_DropOut_Augmentation.add(layers.MaxPooling2D((2, 2)))\n","model_DropOut_Augmentation.add(layers.Flatten())\n","model_DropOut_Augmentation.add(layers.Dense(192, activation='relu'))\n","model_DropOut_Augmentation.add(layers.Dropout(0.4))\n","model_DropOut_Augmentation.add(layers.Dense(64, activation='relu'))\n","model_DropOut_Augmentation.add(layers.Dense(1, activation='sigmoid'))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KnSX_sAzA38F","colab":{}},"source":["model_DropOut_Augmentation.summary()#a summary of the architecture"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q1jWNS6wA38I","colab":{}},"source":["model_DropOut_Augmentation.compile(optimizer=keras.optimizers.RMSprop(lr=0.001),\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy'])\n","#early stopping\n","callback = tf.keras.callbacks.EarlyStopping(monitor='binary_accuracy', patience=10)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S_R8ypMfBLY8","colab_type":"text"},"source":["### Getting the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_wSJ80PY53c3","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DtBEAO3aOIhq"},"source":["## Deleting the baseline"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"266J1nJiOIhv","colab":{}},"source":["#Here i manage the input tensors, removing the baseline patch and the relative labels in order to perform binary classification\n","SubTrainArray_AG =  np.empty_like(train_images_Augmented)\n","SubTrainArray_AG  =  np.delete(train_images_Augmented, np.s_[::2], 0)\n","print(SubTrainArray_AG[1])\n","SubLabelArray_AG  = np.empty_like(train_labels_Augmented)\n","SubLabelArray_AG  =  np.delete(train_labels_Augmented, np.s_[::2], 0)\n","print(SubLabelArray_AG[0:100])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zbZGIDq_OIh5"},"source":["## Editing the label "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JE9mIhgIOIh6","colab":{}},"source":["#Here i manage the input tensor and the label in order to perform binary classification\n","\n","leng = len(SubLabelArray_AG)\n","SubLabelArray_Binary_AG = np.empty_like(SubLabelArray_AG)\n","for i in range(leng):\n","    if SubLabelArray_AG[i] == 1 or SubLabelArray_AG[i] == 2 :\n","      SubLabelArray_Binary_AG[i] = 0\n","    if SubLabelArray_AG[i] == 3 or SubLabelArray_AG[i] == 4:\n","      SubLabelArray_Binary_AG[i] = 1\n","\n","print(SubLabelArray_Binary_AG[0:1000])\n","print(SubLabelArray_Binary_AG[2000:])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ozh6CO8aOIh_"},"source":["## Shuffle"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5isSQlesOIiA","colab":{}},"source":["#eseguo uno shuffle dei dati \n"," from sklearn.utils import shuffle\n","\n"," SubTrainArray_Shuffled_AG, SubLabelArray_Shuffled_AG = shuffle( SubTrainArray_AG, SubLabelArray_Binary_AG, random_state=42)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vLLmznjNJlIq"},"source":["## Modifiyng the test set"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3WcisfvWJlIt","colab":{}},"source":["#Here i manage the TEST data set, removing baseline\n","SubTestArray_Augmented =  np.empty_like(test_images_Augmented)\n","SubTestArray_Augmented =  np.delete(test_images_Augmented, np.s_[::2], 0)\n","\n","SubTestLabelArray_Augmented = np.empty_like(test_lable_Augmented)\n","SubTestLabelArray_Augmented =  np.delete(test_lable_Augmented, np.s_[::2], 0)\n","print(SubTestLabelArray_Augmented[0:1000])\n","print(SubTestLabelArray_Augmented[2000:3000])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SkFn36pwJlIx","colab":{}},"source":["leng = len(SubTestLabelArray_Augmented)\n","SubTestLabelArray_Augmented_Edited = np.empty_like(SubTestLabelArray_Augmented)\n","for i in range(leng):\n","    if SubTestLabelArray_Augmented[i] == 1 or SubTestLabelArray_Augmented[i] == 2:\n","      SubTestLabelArray_Augmented_Edited[i] = 0\n","    if SubTestLabelArray_Augmented[i] == 3 or SubTestLabelArray_Augmented[i] == 4:\n","      SubTestLabelArray_Augmented_Edited[i] = 1\n","\n","print(SubTestLabelArray_Augmented_Edited[0:1000])\n","print(SubTestLabelArray_Augmented_Edited[1000:])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sg7jl7PcMj2y","colab_type":"text"},"source":["## Splitting"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zzw8pCVwMKiq","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","\n","\n","\n","train_data, val_data, train_targets, val_targets = train_test_split(SubTrainArray_Shuffled_AG, SubLabelArray_Shuffled_AG, test_size=0.1, stratify=SubLabelArray_Shuffled_AG)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnaVmCfwnce4","colab_type":"code","colab":{}},"source":["print(train_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4l9wYp7Q0ljK","colab_type":"text"},"source":["## Data Augmentation"]},{"cell_type":"code","metadata":{"id":"9HBTtyQm_dIM","colab_type":"code","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.1, \n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(rescale=1./65535)\n","test_datagen = ImageDataGenerator(rescale=1./65535)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5U_H_CV0HN86","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_data, val_data, train_targets, val_targets = train_test_split(SubTrainArray_Shuffled_AG, SubLabelArray_Shuffled_AG, test_size=0.1, stratify=SubLabelArray_Binary_AG)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJ4-hwwF_yTq","colab_type":"code","colab":{}},"source":["\n","\n","train_generator = train_datagen.flow(\n","        # This is the target directory\n","        train_data,\n","        train_targets,\n","        # All images will be resized to 150x150\n","        batch_size=32\n","        )\n","\n","validation_generator = validation_datagen.flow(\n","        val_data,\n","        val_targets,\n","        batch_size=32)\n","\n","test_generator = test_datagen.flow(\n","        SubTestArray_Augmented,\n","        SubTestLabelArray_Augmented_Edited,\n","        batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jxu7UC0YB8KD","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","augmented_images = [train_generator[0][0][0] for i in range(5)]\n","for data_batch, labels_batch in train_generator:\n","    plt.imshow(np.squeeze(data_batch[3]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[4]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[5]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[1]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[2]), cmap='Greys')\n","    plt.show()\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRqSZIKig9mB","colab_type":"code","colab":{}},"source":["\n","history = model_DropOut_Augmentation.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=2 ,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MndqbMWIlMit"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qMbBSchRlMiw","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = model_DropOut_Augmentation.evaluate_generator(test_generator,steps= len(test_generator))\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IO_TcvZSnXyJ"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z0CdPX7pncS-","colab":{}},"source":["#Save the model!\n","model_DropOut_Augmentation.save(os.path.join(base_dir,'Models/Model_Task_2.1_WithAugmentatin_Definitivo_0.875.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hIUf9vhmncTE","colab":{}},"source":["#Restore the saved model\n","model_Drmodel_DropOut_AugmentationopOut = tf.keras.models.load_model(os.path.join(base_dir,'Models/Model_Task_2.1_WithDROPOUT.h5'))\n","\n","# Show the model architecture\n","model_DropOut.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwIenROhKVup","colab_type":"text"},"source":["# DCCNN version"]},{"cell_type":"markdown","metadata":{"id":"FOu5xgRmfgIb","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"EYlt5EWXLHA7","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.14\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import *\n","from keras.models import Model\n","from keras.layers.core import Dense, Dropout, Flatten\n","from keras.layers.convolutional import MaxPooling2D, Convolution2D\n","\n","img_rows, img_cols = 150, 150\n","\n","nb_filters_1 = 32\n","nb_filters_2 = 64\n","nb_filters_3 = 128\n","nb_conv = 3\n","\n","\n","init = keras.layers.Input(shape=(150,150,1),)\n","\n","fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),  activation='relu')(init)\n","fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), activation='relu')(init)\n","merge1 = keras.layers.Subtract()([fork11, fork12,])\n","maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool1)\n","fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool2)\n","fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool2)\n","merge3 = keras.layers.Subtract()([fork31, fork32,  ]) \n","maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","fork41 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","fork42 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","fork43 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","fork44 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","fork45 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","fork46 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","merge4 = keras.layers.Concatenate()([fork41, fork42, fork43, fork44, fork45, fork46, ]) \n","maxpool4 = keras.layers.MaxPooling2D(strides=(2,2),)(merge4)\n","flatten = keras.layers.Flatten()(maxpool4)\n","dropout = keras.layers.Dropout(0.5)(flatten)\n","dense1 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","dense2 = keras.layers.Dense(64, activation=\"relu\")(dense1)\n","output = keras.layers.Dense(1, activation=\"sigmoid\")(dense2)\n","\n","model = keras.models.Model(init, output)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"teWVW8yvTgXG","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QO5VkCTCVUzE","colab":{}},"source":["model.compile(optimizer='Adam',\n","              loss='binary_crossentropy',\n","              metrics=['binary_accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16_fHIRB6r0m","colab_type":"text"},"source":["## 10-Cross validation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZrOf3qv8VUzJ","colab":{}},"source":["#cross-validation, SUL TRAINING ESTRATTO e shuffled\n","import numpy as np\n","keras.layers.Conv2D?\n","keras.layers.MaxPooling2D?\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","def get_model():\n","\n","  img_rows, img_cols = 150, 150\n","\n","\n","  nb_filters_1 = 32\n","  nb_filters_2 = 64\n","  nb_filters_3 = 128\n","  nb_conv = 3\n","\n","\n","  init = keras.layers.Input(shape=(150,150,1),)\n","\n","  fork11 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv),  activation='relu')(init)\n","  fork12 = keras.layers.Conv2D(nb_filters_1, (nb_conv, nb_conv), activation='relu')(init)\n","  merge1 = keras.layers.Subtract()([fork11, fork12,])\n","  maxpool1 = keras.layers.MaxPooling2D(strides=(2,2),)(merge1)\n","\n","  fork21 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool1)\n","  fork22 = keras.layers.Conv2D(nb_filters_2, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001))(maxpool1)\n","  merge2 = keras.layers.Subtract()([fork21, fork22, ])\n","  maxpool2 = keras.layers.MaxPooling2D(strides=(2,2), )(merge2)\n","\n","  fork31 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool2)\n","  fork32 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001) )(maxpool2)\n","  merge3 = keras.layers.Subtract()([fork31, fork32,  ]) \n","  maxpool3 = keras.layers.MaxPooling2D(strides=(2,2), )(merge3)\n","\n","  fork41 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","  fork42 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","  fork43 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","  fork44 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer= keras.regularizers.l2(0.001))(maxpool3)\n","  fork45 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","  fork46 = keras.layers.Conv2D(nb_filters_3, (nb_conv, nb_conv), activation='relu',kernel_regularizer=keras.regularizers.l2(0.001 ))(maxpool3)\n","  merge4 = keras.layers.Concatenate()([fork41, fork42, fork43, fork44, fork45, fork46, ]) \n","  maxpool4 = keras.layers.MaxPooling2D(strides=(2,2),)(merge4)\n","  flatten = keras.layers.Flatten()(maxpool4)\n","  dropout = keras.layers.Dropout(0.5)(flatten)\n","  dense1 = keras.layers.Dense(128, activation=\"relu\")(dropout)\n","  dense2 = keras.layers.Dense(64, activation=\"relu\")(dense1)\n","  output = keras.layers.Dense(1, activation=\"sigmoid\")(dense2)\n","\n","  Cross_model = keras.models.Model(init, output)\n","\n","  return Cross_model\n","\n","def plot():\n","  acc = history.history['binary_accuracy']\n","  val_acc = history.history['val_binary_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  epochs = range(len(acc))\n","\n","  plt.plot(epochs, acc, 'bo', label='Training acc')\n","  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n","  plt.title('Training and validation accuracy')\n","  plt.legend()\n","\n","  plt.figure()\n","\n","  plt.plot(epochs, loss, 'bo', label='Training loss')\n","  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","  plt.title('Training and validation loss')\n","  plt.legend()\n","\n","  plt.show()\n","  return\n","\n","\n","############################# CROSS VALIDATION ######################\n","\n","k = 10\n","num_val_samples = len(SubTrainArray_Shuffled) // k\n","num_epochs = 40\n","History_Arr =[]\n","Cross_model = get_model()\n","Cross_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['binary_accuracy'])\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=8)\n","for i in range(k):\n","  print('processing fold #', i)\n","  val_data = SubTrainArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  val_targets = SubLabelArray_Shuffled[i * num_val_samples: (i + 1) * num_val_samples]\n","  partial_train_data = np.concatenate([SubTrainArray_Shuffled[:i * num_val_samples],SubTrainArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  partial_train_targets = np.concatenate([SubLabelArray_Shuffled[:i * num_val_samples],SubLabelArray_Shuffled[(i + 1) * num_val_samples:]],axis=0)\n","  history = Cross_model.fit(partial_train_data, partial_train_targets,validation_data=(val_data, val_targets), callbacks=[callback], epochs=num_epochs, batch_size=32)\n","  History_Arr.append(history)\n","  Cross_model = get_model()\n","  Cross_model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['binary_accuracy'])\n","  plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kOVLcE7FawZV"},"source":["## FITTING COMPLETO\n","Dopo la cross validation, fitto su tutto il DATASET"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HHYZbLK_awZW","colab":{}},"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n","history = model.fit(SubTrainArray_Shuffled, SubLabelArray_Shuffled, epochs=50,callbacks=[callback], batch_size=32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DCF_jc4dawZZ"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"90GmkU9XawZa","colab":{}},"source":["#evaluate the model\n","\n","\n","test_loss, test_acc = model.evaluate(SubTestArray, SubTestLabelArray_Edited,batch_size=32, verbose= 1)\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NRBaz71Z90C7"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hBQIiTvg90C9","colab":{}},"source":["#Save the model!\n","model.save(os.path.join(base_dir,'Models/Model_Task_2.1_DCCNN_NoAugmentaton_0.82.h5'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cr4HtZO4KvP7","colab":{}},"source":["#Here i manage the input tensors, removing the baseline patch and the relative labels in order to perform binary classification\n","SubTrainArray_AG =  np.empty_like(train_images_Augmented)\n","SubTrainArray_AG  =  np.delete(train_images_Augmented, np.s_[::2], 0)\n","print(SubTrainArray_AG[1])\n","SubLabelArray_AG  = np.empty_like(train_labels_Augmented)\n","SubLabelArray_AG  =  np.delete(train_labels_Augmented, np.s_[::2], 0)\n","print(SubLabelArray_AG[0:100])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y13vQECArLDH"},"source":["## Data Augmentation"]},{"cell_type":"markdown","metadata":{"id":"vAdan8UkPLPz","colab_type":"text"},"source":["## Getting the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lSUwpH2rr_2P","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","def load_training():\n","  train_images = np.load(os.path.join(base_dir,'Tensors/train_tensor.npy'))\n","  train_labels = np.load(os.path.join(base_dir,'Tensors/train_labels.npy'))\n","  test_images = np.load(os.path.join(base_dir,'Tensors/public_test_tensor.npy'))\n","  test_lables = np.load(os.path.join(base_dir,'Tensors/public_test_labels.npy'))\n","  return train_images,train_labels, test_images, test_lables\n"," \n","train_images_Augmented, train_labels_Augmented, test_images_Augmented, test_lable_Augmented = load_training()\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBIVeXmhI5e1","colab_type":"code","colab":{}},"source":["#reshape per poter fittare la CNN\n","train_images_Augmented = train_images_Augmented.reshape((5352, 150, 150,1))\n","\n","test_images_Augmented = test_images_Augmented.reshape((672, 150, 150,1))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5bawezJxr_2V"},"source":["### Deleting the baseline"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qmuEBIDwr_2W","colab":{}},"source":["#Here i manage the input tensors, removing the baseline patch and the relative labels in order to perform binary classification\n","SubTrainArray_AG =  np.empty_like(train_images_Augmented)\n","SubTrainArray_AG  =  np.delete(train_images_Augmented, np.s_[::2], 0)\n","print(SubTrainArray_AG[1])\n","SubLabelArray_AG  = np.empty_like(train_labels_Augmented)\n","SubLabelArray_AG  =  np.delete(train_labels_Augmented, np.s_[::2], 0)\n","print(SubLabelArray_AG[0:100])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Iwg-nuxsr_2Y"},"source":["### Editing the label "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"exMQ1Dxpr_2a","colab":{}},"source":["#Here i manage the input tensor and the label in order to perform binary classification\n","\n","leng = len(SubLabelArray_AG)\n","SubLabelArray_Binary_AG = np.empty_like(SubLabelArray_AG)\n","for i in range(leng):\n","    if SubLabelArray_AG[i] == 1 or SubLabelArray_AG[i] == 2 :\n","      SubLabelArray_Binary_AG[i] = 0\n","    if SubLabelArray_AG[i] == 3 or SubLabelArray_AG[i] == 4:\n","      SubLabelArray_Binary_AG[i] = 1\n","\n","print(SubLabelArray_Binary_AG[0:1000])\n","print(SubLabelArray_Binary_AG[2000:])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"W0rSIGGBr_2f"},"source":["### Shuffle"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uObyTXrqr_2f","colab":{}},"source":["#eseguo uno shuffle dei dati \n"," from sklearn.utils import shuffle\n","\n"," SubTrainArray_Shuffled_AG, SubLabelArray_Shuffled_AG = shuffle( SubTrainArray_AG, SubLabelArray_Binary_AG, random_state=42)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3FK4veC5ZFUu"},"source":["## Modifiyng the test set"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R9x_GPuTZFUr"},"source":["### Deleting BaseLine"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OSgn38gRZFUm","colab":{}},"source":["#Here i manage the TEST data set, removing baseline\n","SubTestArray_Augmented =  np.empty_like(test_images_Augmented)\n","SubTestArray_Augmented =  np.delete(test_images_Augmented, np.s_[::2], 0)\n","\n","SubTestLabelArray_Augmented = np.empty_like(test_lable_Augmented)\n","SubTestLabelArray_Augmented =  np.delete(test_lable_Augmented, np.s_[::2], 0)\n","print(SubTestArray_Augmented[0])\n","print(SubTestArray_Augmented.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sMEAQZRnZFUk"},"source":["### Editing the labels "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"D1W8gFpOZFUY","colab":{}},"source":["#Here i modify the label\n","SubTestLabelArray_Edited_Augmented = np.zeros_like(SubTestLabelArray_Augmented)\n","\n","leng = len(SubTestLabelArray_Augmented)\n","for i in range(leng):\n","    if SubTestLabelArray_Augmented[i] == 1 or SubTestLabelArray_Augmented[i] == 2 :\n","      SubTestLabelArray_Edited_Augmented[i] = 0\n","    if SubTestLabelArray_Augmented[i] == 3 or SubTestLabelArray_Augmented[i] == 4:\n","      SubTestLabelArray_Edited_Augmented[i] = 1\n","\n","print(SubTestLabelArray_Edited_Augmented[0:1000])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"euTFbJ72r_2r"},"source":["## Data Augmentation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9DxCocQ9r_2r","colab":{}},"source":["from PIL import Image\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_datagen = ImageDataGenerator(\n","      rescale=1./65535,\n","      rotation_range=360,\n","      zoom_range=0.2, \n","      horizontal_flip=True,\n","      vertical_flip=True,\n","      fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(rescale=1./65535)\n","test_datagen = ImageDataGenerator(rescale=1./65535)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AjbVl9TDr_2t","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","\n","train_data, val_data, train_targets, val_targets = train_test_split(SubTrainArray_Shuffled_AG, SubLabelArray_Shuffled_AG, test_size=0.1, stratify=SubLabelArray_Binary_AG)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a297osWBr_2v","colab":{}},"source":["\n","\n","train_generator = train_datagen.flow(\n","        # This is the target directory\n","        train_data,\n","        train_targets,\n","        batch_size=32\n","        )\n","\n","validation_generator = validation_datagen.flow(\n","        val_data,\n","        val_targets,\n","        batch_size=32)\n","\n","test_generator = test_datagen.flow(\n","        SubTestArray_Augmented,\n","        SubTestLabelArray_Edited_Augmented,\n","        batch_size=32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FRr8M0Rir_2x","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","augmented_images = [train_generator[0][0][0] for i in range(5)]\n","for data_batch, labels_batch in train_generator:\n","    plt.imshow(np.squeeze(data_batch[6]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[2]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[0]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[1]), cmap='Greys')\n","    plt.show()\n","    plt.imshow(np.squeeze(data_batch[2]), cmap='Greys')\n","    plt.show()\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sB-1IWAYKo8O","colab":{}},"source":["\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=2408//32,\n","      epochs=40,\n","      validation_data=validation_generator,\n","      validation_steps=268//32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-Xyab9KVKo8S"},"source":["## Evalutate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1rg_-ioNKo8S","colab":{}},"source":["#evaluate the model\n","\n","test_loss, test_acc = model.evaluate_generator(test_generator,steps= len(test_generator))\n","print(test_loss)\n","print(test_acc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1ZFUjCuXyokd"},"source":["## Load OR Save the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"G5UidyC9yokg","colab":{}},"source":["#Save the model!\n","model.save(os.path.join(base_dir,'Models/Model_Task_2.1_DCCNN_Definitivo_Augmentaton_0.9017.h5'))"],"execution_count":0,"outputs":[]}]}